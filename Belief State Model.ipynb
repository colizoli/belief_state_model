{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model predictions - Colizoli, de Gee, Urai & Donner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary packages from Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical decision theory - competing model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigma is the noise in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see Estimate Sigma in this repository\n",
    "sigma = 1/7.5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate 'stimuli' - values of motion coherence moving either up or down [1 or -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_coh = np.arange(-.5,.5,0.01) # extreme values of motion coherence, 1=100% coherent motion      \n",
    "model_evs = np.repeat(model_coh, 10000) # repeat each coherence value 10000 times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The internal decision variable, dv, is a 'noisy' version of the stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dvs = model_evs + np.random.normal(0, sigma, len(model_evs)) # add noise to the decision variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The choice on each trial is the sign of the noisy decision variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_choice = np.sign(model_dvs) # the choice is the sign of the dv, up or down\n",
    "model_correct = model_choice == np.sign(model_evs) # correct choice when sign dv = sign of ev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equation 3 transforms the dv to the probability of a correct response based on the noise in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dv2conf(x,sigma):\n",
    "    from scipy.special import erf\n",
    "    return 0.5 * (1 + erf(x / (np.sqrt(2)*sigma)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The difference in the two models is here: whether confidence is a function of the external or internal decision variable (Equation 1 vs. Equation 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0 # criterion set to 0, assuming no bias\n",
    "model_confidence_belief = dv2conf(np.abs(model_dvs-c),sigma); # belief state model (model1)\n",
    "model_confidence = dv2conf(np.abs(model_evs-c),sigma); # stimulus state model (model2)\n",
    "# see cell 'Averaging' for n trials for each conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the feedback and prediction error for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feedback = model_correct; # feedback = 0 or 1\n",
    "model_rpe_belief = model_feedback - model_confidence_belief; # belief state model, see Eq. 5 and cell 'Averaging'\n",
    "model_rpe = model_feedback - model_confidence; # stimulus state model, see Eq. 5 and cell 'Averaging'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_levels_dvs = np.abs(model_dvs) # collapse across up/down directions, because difficulty is orthogonal to direction\n",
    "\n",
    "model1 = pd.DataFrame() # belief state model\n",
    "model1['evs'] = model_evs\n",
    "model1['dvs'] = model_dvs\n",
    "model1['ev_levels'] = np.abs(model_evs) \n",
    "model1['correct'] = model_correct\n",
    "model1['confidence'] = model_confidence_belief\n",
    "model1['uncertainty'] = 1 - model_confidence_belief\n",
    "model1['pe'] = model_rpe_belief\n",
    "model1['pe_inv'] = 1 - model_rpe_belief \n",
    "\n",
    "model2 = pd.DataFrame() # stimulus state model\n",
    "model2['evs'] = model_evs\n",
    "model2['dvs'] = model_dvs\n",
    "model2['ev_levels'] = np.abs(model_evs) \n",
    "model2['correct'] = model_correct\n",
    "model2['confidence'] = model_confidence \n",
    "model2['uncertainty'] = 1 - model_confidence\n",
    "model2['pe'] = model_rpe  \n",
    "model2['pe_inv'] = 1 - model_rpe \n",
    "\n",
    "# model1.to_csv('model1.csv') # uncomment to save model predictions as CSV files\n",
    "# model2.to_csv('model2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set parameters for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'font.size': 12.0,\n",
       " 'axes.labelsize': 7.0,\n",
       " 'axes.titlesize': 7.0,\n",
       " 'xtick.labelsize': 6.0,\n",
       " 'ytick.labelsize': 6.0,\n",
       " 'legend.fontsize': 6.0,\n",
       " 'grid.linewidth': 1.0,\n",
       " 'lines.linewidth': 1.75,\n",
       " 'patch.linewidth': 0.3,\n",
       " 'lines.markersize': 7.0,\n",
       " 'lines.markeredgewidth': 0.0,\n",
       " 'xtick.major.width': 1.0,\n",
       " 'ytick.major.width': 1.0,\n",
       " 'xtick.minor.width': 0.5,\n",
       " 'ytick.minor.width': 0.5,\n",
       " 'xtick.major.pad': 7.0,\n",
       " 'ytick.major.pad': 7.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "sns.set(style='ticks', font='Arial', font_scale=1, rc={\n",
    "    'axes.linewidth': 1,\n",
    "    'axes.labelsize': 7, \n",
    "    'axes.titlesize': 7,\n",
    "    'xtick.labelsize': 6,\n",
    "    'ytick.labelsize': 6,\n",
    "    'legend.fontsize': 6,\n",
    "    'xtick.major.width': 1,\n",
    "    'ytick.major.width': 1,\n",
    "    'text.color': 'Black',\n",
    "    'axes.labelcolor':'Black',\n",
    "    'xtick.color':'Black',\n",
    "    'ytick.color':'Black',} )\n",
    "sns.plotting_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Plotting in the direction of uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCERTAINTY\n",
    "decision_dv = 'uncertainty'\n",
    "feedback_dv = 'pe_inv'\n",
    "# y lims\n",
    "dec = [0,0.5]\n",
    "feed = [0.4,2]\n",
    "inter = [-0.5,0.5]\n",
    "yticksfeed = [0.4,0.8,1.2,1.6,2]\n",
    "# xlims\n",
    "xdiff = [0,0.5]\n",
    "xdifflabels = ['Hard','Easy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set number of bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 10\n",
    "bin_labels = range(1,nbins+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Averaging: group predictions by stimulus/coherence levels and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# averaging over n in Eq.1 and Eq.2, \n",
    "# where n is the number of trials in each ev_level separately for error and correct responses\n",
    "m1_dec = model1.groupby(['ev_levels','correct'])[decision_dv].mean()\n",
    "m2_dec = model2.groupby(['ev_levels','correct'])[decision_dv].mean()\n",
    "\n",
    "m1_feed = model1.groupby(['ev_levels','correct'])[feedback_dv].mean()\n",
    "m2_feed = model2.groupby(['ev_levels','correct'])[feedback_dv].mean()\n",
    "\n",
    "m1_dec = m1_dec.reset_index() # resets indices to accessible columns\n",
    "m2_dec = m2_dec.reset_index()\n",
    "m1_feed = m1_feed.reset_index()\n",
    "m2_feed = m2_feed.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a figure (2 x 3 panels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(6,4))\n",
    "        \n",
    "# for interaction plots\n",
    "ind = [0.3,0.5]\n",
    "xlim = [0.275,0.575]\n",
    "\n",
    "# get xlabels and color scheme for two conditions\n",
    "ls = 'interact' # corresponding to lines\n",
    "leg_label = ['Decision','Feedback']\n",
    "colors = ['blue','purple']\n",
    "alphas = [1,1]\n",
    "xticklabels = ['Decision','Feedback'] # for interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Belief State Model - Decision interval (subplot 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Belief State Model')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ax = fig.add_subplot(231)\n",
    "        \n",
    "# Make equal size bins for the error trials (correct==False)\n",
    "# Average coherence and confidence in each bin\n",
    "valuelist = [False]\n",
    "F = m1_dec[m1_dec.correct.isin(valuelist)] # ev_levels that have errors\n",
    "valueList = np.array(F['ev_levels'])\n",
    "T =  m1_dec[m1_dec['correct']] # only true values\n",
    "T = T[T.ev_levels.isin(valueList)] # get only those ev_levels that are in m1_dec_F\n",
    "\n",
    "F = F.reset_index() \n",
    "T = T.reset_index() \n",
    "\n",
    "# Error\n",
    "bins_F = pd.qcut(F['ev_levels'], nbins, labels=bin_labels, retbins=True) \n",
    "F['bins'] = np.array(bins_F[0]) # tupel, get first element out\n",
    "F_binned = F.groupby(['bins'])['ev_levels', decision_dv].mean()\n",
    "\n",
    "# Correct\n",
    "bins_T = pd.qcut(T['ev_levels'], nbins, labels=bin_labels, retbins=True) \n",
    "T['bins'] = np.array(bins_T[0]) # tupel, get first element out\n",
    "T_binned = T.groupby(['bins'])['ev_levels', decision_dv].mean()\n",
    "\n",
    "# Save for interaction plot\n",
    "easy_error_dec = F_binned.iloc[-1,-1] # last level, confidence/pe\n",
    "hard_error_dec = F_binned.iloc[0,-1] # first level, confidence/pe\n",
    "easy_correct_dec = T_binned.iloc[-1,-1] # last level, confidence/pe\n",
    "hard_correct_dec = T_binned.iloc[0,-1] # first level, confidence/pe\n",
    "\n",
    "### LINE GRAPH ###        \n",
    "ax.plot(T_binned['ev_levels'],T_binned[decision_dv], label='Correct', color='green', alpha=1)\n",
    "ax.plot(F_binned['ev_levels'],F_binned[decision_dv], label='Error', color='red', alpha=1)\n",
    "\n",
    "# Subplot parameters\n",
    "ax.set_ylim(dec)\n",
    "ax.set_xticks(xdiff)\n",
    "ax.set_xticklabels(xdifflabels)\n",
    "ax.legend(loc='best', fontsize=4)\n",
    "ax.set_ylabel('Decision uncertainty')\n",
    "ax.set_xlabel('Task difficulty')\n",
    "ax.set_title('Belief State Model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Belief State Model - Feedback interval (subplot 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Belief State Model')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ax = fig.add_subplot(232)\n",
    "\n",
    "# Make equal size bins for the error trials (correct==False)\n",
    "# Average coherence and confidence in each bin\n",
    "valuelist = [False]\n",
    "F = m1_feed[m1_feed.correct.isin(valuelist)] # ev_levels that have errors\n",
    "valueList = np.array(F['ev_levels'])\n",
    "T =  m1_feed[m1_feed['correct']] # only true values\n",
    "T = T[T.ev_levels.isin(valueList)] # get only those ev_levels that are in m1_dec_F\n",
    "\n",
    "F = F.reset_index() \n",
    "T = T.reset_index() \n",
    "\n",
    "# Error\n",
    "bins_F = pd.qcut(F['ev_levels'], nbins, labels=bin_labels, retbins=True) \n",
    "F['bins'] = np.array(bins_F[0]) # tupel, get first element out\n",
    "F_binned = F.groupby(['bins'])['ev_levels', feedback_dv].mean()\n",
    "\n",
    "# Correct\n",
    "bins_T = pd.qcut(T['ev_levels'], nbins, labels=bin_labels, retbins=True) \n",
    "T['bins'] = np.array(bins_T[0]) # tupel, get first element out\n",
    "T_binned = T.groupby(['bins'])['ev_levels', feedback_dv].mean()\n",
    "\n",
    "# save for interaction plot\n",
    "easy_error_feed = F_binned.iloc[-1,-1] # last level, confidence/pe\n",
    "hard_error_feed = F_binned.iloc[0,-1] # first level, confidence/pe\n",
    "easy_correct_feed = T_binned.iloc[-1,-1] # last level, confidence/pe\n",
    "hard_correct_feed = T_binned.iloc[0,-1] # first level, confidence/pe\n",
    "\n",
    "### LINE GRAPH ###        \n",
    "ax.plot(T_binned['ev_levels'],T_binned[feedback_dv], label='Correct', color='green', alpha=1)\n",
    "ax.plot(F_binned['ev_levels'],F_binned[feedback_dv], label='Error', color='red', alpha=1)\n",
    "\n",
    "# Subplot parameters\n",
    "ax.set_xticks(xdiff)\n",
    "ax.set_ylim(feed)\n",
    "ax.set_yticks(yticksfeed)\n",
    "ax.set_xticklabels(xdifflabels)\n",
    "ax.legend(loc='best', fontsize=4)\n",
    "ax.set_ylabel('1 - prediction error')\n",
    "ax.set_xlabel('Task difficulty')\n",
    "ax.set_title('Belief State Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Belief State Model - Interaction (subplot 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Belief State Model')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ax = fig.add_subplot(233)\n",
    "ax.axhline(0, lw=1, alpha=1, color = 'k') # Add horizontal line at t=0    \n",
    "\n",
    "easy_error_dec = np.round(easy_error_dec,2) # round to 2 decimal places\n",
    "hard_error_dec = np.round(hard_error_dec,2)\n",
    "easy_correct_dec = np.round(easy_correct_dec,2)\n",
    "hard_correct_dec = np.round(hard_correct_dec,2)\n",
    "\n",
    "easy_error_feed = np.round(easy_error_feed,2)\n",
    "hard_error_feed = np.round(hard_error_feed,2)\n",
    "easy_correct_feed = np.round(easy_correct_feed,2)\n",
    "hard_correct_feed = np.round(hard_correct_feed,2)\n",
    "\n",
    "interaction_decision = (easy_error_dec - easy_correct_dec) - (hard_error_dec - hard_correct_dec)\n",
    "interaction_feed = (easy_error_feed - easy_correct_feed) - (hard_error_feed - hard_correct_feed)\n",
    "\n",
    "MEANS = [interaction_decision,interaction_feed]\n",
    "\n",
    "for x,m in enumerate(MEANS):\n",
    "    ax.plot(ind[x],MEANS[x], marker='o', color=colors[x], label=leg_label[x], alpha=alphas[x])\n",
    "\n",
    "# Subplot parameters\n",
    "ax.set_xticks(ind)\n",
    "ax.set_ylim(inter)\n",
    "ax.set_xticklabels(xticklabels)\n",
    "ax.legend([])\n",
    "ax.set_ylabel('Interaction term')\n",
    "ax.set_title('Belief State Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stimulus State Model - Decision interval (subplot 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Stimulus State Model')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ax = fig.add_subplot(234)\n",
    "\n",
    "# Make equal size bins for the error trials (correct==False)\n",
    "# Average coherence and confidence in each bin\n",
    "valuelist = [False]\n",
    "F = m2_dec[m2_dec.correct.isin(valuelist)] # ev_levels that have errors\n",
    "valueList = np.array(F['ev_levels'])\n",
    "T =  m2_dec[m2_dec['correct']] # only true values\n",
    "T = T[T.ev_levels.isin(valueList)] # get only those ev_levels that are in m1_dec_F\n",
    "\n",
    "F = F.reset_index() \n",
    "T = T.reset_index() \n",
    "\n",
    "# Error\n",
    "bins_F = pd.qcut(F['ev_levels'], nbins, labels=bin_labels, retbins=True) \n",
    "F['bins'] = np.array(bins_F[0]) # tupel, get first element out\n",
    "F_binned = F.groupby(['bins'])['ev_levels', decision_dv].mean()\n",
    "\n",
    "# Correct\n",
    "bins_T = pd.qcut(T['ev_levels'], nbins, labels=bin_labels, retbins=True) \n",
    "T['bins'] = np.array(bins_T[0]) # tupel, get first element out\n",
    "T_binned = T.groupby(['bins'])['ev_levels', decision_dv].mean()\n",
    "\n",
    "# Save for interaction plot\n",
    "easy_error_dec = F_binned.iloc[-1,-1] # last level, confidence/pe\n",
    "hard_error_dec = F_binned.iloc[0,-1] # first level, confidence/pe\n",
    "easy_correct_dec = T_binned.iloc[-1,-1] # last level, confidence/pe\n",
    "hard_correct_dec = T_binned.iloc[0,-1] # first level, confidence/pe\n",
    "\n",
    "### LINE GRAPH ###        \n",
    "ax.plot(T_binned['ev_levels'],T_binned[decision_dv], label='Correct', color='green', alpha=1)\n",
    "ax.plot(F_binned['ev_levels'],F_binned[decision_dv], label='Error', color='red', alpha=1)\n",
    "\n",
    "# Subplot parameters\n",
    "ax.set_xticks(xdiff)\n",
    "ax.set_ylim(dec)\n",
    "ax.set_xticklabels(xdifflabels)\n",
    "ax.legend(loc='best', fontsize=4)\n",
    "ax.set_ylabel('Decision uncertainty')\n",
    "ax.set_xlabel('Task difficulty')\n",
    "ax.set_title('Stimulus State Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stimulus State Model - Feedback interval (subplot 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Stimulus State Model')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ax = fig.add_subplot(235)\n",
    "\n",
    "# Make equal size bins for the error trials (correct==False)\n",
    "# Average coherence and confidence in each bin\n",
    "valuelist = [False]\n",
    "F = m2_feed[m2_feed.correct.isin(valuelist)] # ev_levels that have errors\n",
    "valueList = np.array(F['ev_levels'])\n",
    "T =  m2_feed[m2_feed['correct']] # only true values\n",
    "T = T[T.ev_levels.isin(valueList)] # get only those ev_levels that are in m1_dec_F\n",
    "\n",
    "F = F.reset_index() \n",
    "T = T.reset_index() \n",
    "\n",
    "# Error\n",
    "bins_F = pd.qcut(F['ev_levels'], nbins, labels=bin_labels, retbins=True) \n",
    "F['bins'] = np.array(bins_F[0]) # tupel, get first element out\n",
    "F_binned = F.groupby(['bins'])['ev_levels', feedback_dv].mean()\n",
    "\n",
    "# Correct\n",
    "bins_T = pd.qcut(T['ev_levels'], nbins, labels=bin_labels, retbins=True) \n",
    "T['bins'] = np.array(bins_T[0]) # tupel, get first element out\n",
    "T_binned = T.groupby(['bins'])['ev_levels', feedback_dv].mean()\n",
    "\n",
    "# Save for interaction plot\n",
    "easy_error_feed = F_binned.iloc[-1,-1] # last level, confidence/pe\n",
    "hard_error_feed = F_binned.iloc[0,-1] # first level, confidence/pe\n",
    "easy_correct_feed = T_binned.iloc[-1,-1] # last level, confidence/pe\n",
    "hard_correct_feed = T_binned.iloc[0,-1] # first level, confidence/pe\n",
    "\n",
    "### LINE GRAPH ###        \n",
    "ax.plot(T_binned['ev_levels'],T_binned[feedback_dv], label='Correct', color='green', alpha=1)\n",
    "ax.plot(F_binned['ev_levels'],F_binned[feedback_dv], label='Error', color='red', alpha=1)\n",
    "\n",
    "# Subplot parameters\n",
    "ax.set_xticks(xdiff)\n",
    "ax.set_ylim(feed)\n",
    "ax.set_yticks(yticksfeed)\n",
    "ax.set_xticklabels(xdifflabels)\n",
    "ax.legend(loc='best', fontsize=4)\n",
    "ax.set_ylabel('1 - prediction error')\n",
    "ax.set_xlabel('Task difficulty')\n",
    "ax.set_title('Stimulus State Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stimulus State Model - Interaction (subplot 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Stimulus State Model')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ax = fig.add_subplot(236)\n",
    "ax.axhline(0, lw=1, alpha=1, color = 'k') # Add horizontal line at t=0\n",
    "\n",
    "easy_error_dec = np.round(easy_error_dec,2)\n",
    "hard_error_dec = np.round(hard_error_dec,2)\n",
    "easy_correct_dec = np.round(easy_correct_dec,2)\n",
    "hard_correct_dec = np.round(hard_correct_dec,2)\n",
    "\n",
    "easy_error_feed = np.round(easy_error_feed,2)\n",
    "hard_error_feed = np.round(hard_error_feed,2)\n",
    "easy_correct_feed = np.round(easy_correct_feed,2)\n",
    "hard_correct_feed = np.round(hard_correct_feed,2)\n",
    "\n",
    "interaction_decision = (easy_error_dec - easy_correct_dec) - (hard_error_dec - hard_correct_dec)\n",
    "interaction_feed = (easy_error_feed - easy_correct_feed) - (hard_error_feed - hard_correct_feed)\n",
    "\n",
    "MEANS = [interaction_decision,interaction_feed]\n",
    "\n",
    "for x,m in enumerate(MEANS):\n",
    "    ax.plot(ind[x],MEANS[x], marker='o', color=colors[x], label=leg_label[x], alpha=alphas[x])\n",
    "\n",
    "# Subplot parameters\n",
    "ax.set_xticks(ind)\n",
    "ax.set_ylim(inter)\n",
    "ax.set_xticklabels(xticklabels)\n",
    "ax.legend([])\n",
    "ax.set_ylabel('Interaction term')\n",
    "ax.set_title('Stimulus State Model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Competing Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.despine(offset=10, trim=True)\n",
    "plt.tight_layout()\n",
    "# fig.savefig('model_predictions.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
